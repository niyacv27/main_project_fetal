# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier  
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, log_loss
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import VotingClassifier
import shap
from collections import Counter

# Load dataset
data = pd.read_csv('fetal_health.csv')

# Remove duplicate rows
data = data.drop_duplicates()

# Separate features and target
X = data.drop('fetal_health', axis=1)
y = data['fetal_health']

# Check class distribution before applying SMOTE
print("Class Distribution Before SMOTE:\n", y.value_counts())

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
y_resampled = y_resampled.astype(int)

# Check class distribution after applying SMOTE
print("\nClass Distribution After SMOTE:\n", y_resampled.value_counts())

# Visualizing Class Distribution
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.countplot(x=y, ax=axes[0])
axes[0].set_title("Class Distribution Before SMOTE")
axes[0].set_xlabel("Fetal Health Class")
axes[0].set_ylabel("Count")

sns.countplot(x=y_resampled, ax=axes[1])
axes[1].set_title("Class Distribution After SMOTE")
axes[1].set_xlabel("Fetal Health Class")
axes[1].set_ylabel("Count")

plt.tight_layout()
plt.show()



# Fix the labels for XGBoost
y_resampled = y_resampled - 1  # Shift labels from [1, 2, 3] to [0, 1, 2]

# Split data into Train (70%), Validation (15%), and Test (15%)
X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Model Initialization
xgb = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)
rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
knn = KNeighborsClassifier(n_neighbors=15)
gb = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, random_state=42)

models = {'XGBoost': xgb, 'Random Forest': rf, 'KNN': knn, 'Gradient Boosting': gb}

# Model Training and Evaluation
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    
    train_pred = model.predict(X_train_scaled)
    val_pred = model.predict(X_val_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_accuracy = model.score(X_train_scaled, y_train)
    val_accuracy = model.score(X_val_scaled, y_val)
    test_accuracy = model.score(X_test_scaled, y_test)

    train_loss = log_loss(y_train, model.predict_proba(X_train_scaled))
    val_loss = log_loss(y_val, model.predict_proba(X_val_scaled))
    test_loss = log_loss(y_test, model.predict_proba(X_test_scaled))

    print(f"\n{name} Model Performance:")
    print(f"Train Accuracy: {train_accuracy * 100:.2f}%")
    print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")
    print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

    print(f"Train Loss: {train_loss:.4f}")
    print(f"Validation Loss: {val_loss:.4f}")
    print(f"Test Loss: {test_loss:.4f}")

    print(f"\n{name} Classification Report:\n", classification_report(y_test, test_pred))
    
    sns.heatmap(confusion_matrix(y_test, test_pred), annot=True, fmt='d')
    plt.title(f"Confusion Matrix for {name}")
    plt.show()

# Voting Classifier (Ensemble Model)
ensemble_model = VotingClassifier(estimators=[
    ('xgb', xgb),
    ('rf', rf),
    ('knn', knn),
    ('gb', gb)
], voting='soft')

ensemble_model.fit(X_train_scaled, y_train)

# Predictions
ensemble_train_pred = ensemble_model.predict(X_train_scaled)
ensemble_val_pred = ensemble_model.predict(X_val_scaled)
ensemble_test_pred = ensemble_model.predict(X_test_scaled)

# Accuracy of Ensemble Model
ensemble_train_accuracy = ensemble_model.score(X_train_scaled, y_train)
ensemble_val_accuracy = ensemble_model.score(X_val_scaled, y_val)
ensemble_test_accuracy = ensemble_model.score(X_test_scaled, y_test)

# Loss of Ensemble Model
ensemble_train_loss = log_loss(y_train, ensemble_model.predict_proba(X_train_scaled))
ensemble_val_loss = log_loss(y_val, ensemble_model.predict_proba(X_val_scaled))
ensemble_test_loss = log_loss(y_test, ensemble_model.predict_proba(X_test_scaled))

print("\nEnsemble Model Performance:")
print(f"Train Accuracy: {ensemble_train_accuracy * 100:.2f}%")
print(f"Validation Accuracy: {ensemble_val_accuracy * 100:.2f}%")
print(f"Test Accuracy: {ensemble_test_accuracy * 100:.2f}%")

print(f"Train Loss: {ensemble_train_loss:.4f}")
print(f"Validation Loss: {ensemble_val_loss:.4f}")
print(f"Test Loss: {ensemble_test_loss:.4f}")

print("\nEnsemble Model Report:\n", classification_report(y_test, ensemble_test_pred))

# Predicting on a new data point (single instance)
new_data = pd.DataFrame({
    'baseline value': [133],  # Higher baseline value, could be indicative of a different class
    'accelerations': [0.003],  # Acceleration might be higher for class 2
    'fetal_movement': [0],  # Slightly more fetal movement
    'uterine_contractions': [0.008],  # Uterine contractions might be more frequent for class 2
    'light_decelerations': [0.003],  # Slightly more decelerations
    'severe_decelerations': [0],  # Mild severe deceleration
    'prolongued_decelerations': [0],  # Prolonged decelerations slightly higher
    'abnormal_short_term_variability': [16],  # Short term variability higher
    'mean_value_of_short_term_variability': [2.1],  # Increased mean value
    'percentage_of_time_with_abnormal_long_term_variability': [0],  # More time with abnormal long-term variability
    'mean_value_of_long_term_variability': [13.4],  # Increased mean value for long-term variability
    'histogram_width': [130],  # Wider histogram could indicate different characteristics
    'histogram_min': [68],
    'histogram_max': [198],  # Higher maximum could indicate a different class
    'histogram_number_of_peaks': [5],  # More peaks in the histogram
    'histogram_number_of_zeroes': [1],  # More zeros might correlate with different patterns
    'histogram_mode': [141],  # Mode might be higher
    'histogram_mean': [135],  # Higher mean value
    'histogram_median': [138],  # Higher median
    'histogram_variance': [13],  # More variance
    'histogram_tendency': [0]  # Slightly more tendency
})

# Scale the new data using the same scaler
new_data_scaled = scaler.transform(new_data)

# Use the Ensemble Model to predict the class for the new data
ensemble_predictions = ensemble_model.predict(new_data_scaled)

# Define the mapping for class labels (starting from 1 instead of 0)
class_labels = {1: 'Normal', 2: 'Suspect', 3: 'Pathological'}

# Get the class number and shift back to [1, 2, 3]
predicted_class = ensemble_predictions[0] + 1  
predicted_class_name = class_labels[predicted_class]

print(f"New Data Prediction: class {predicted_class} , {predicted_class_name}")

# Print the prediction probabilities
ensemble_probabilities = ensemble_model.predict_proba(new_data_scaled)
print(f"New Data Prediction Probabilities: {ensemble_probabilities[0]}")


# ‚úÖ Compute SHAP values using KernelExplainer for kNN models
shap_values_list = []

for name, model in models.items():
    explainer = shap.KernelExplainer(model.predict, shap.sample(X_train_scaled, 100))
    shap_values = explainer.shap_values(new_data_scaled)
    shap_values_list.append(shap_values)

# ‚úÖ Aggregate SHAP values across models (mean absolute SHAP importance)
mean_shap_values = np.abs(np.mean(shap_values_list, axis=0))[0]
feature_names = new_data.columns

# ‚úÖ Sort and Display Top Features (SHAP)
shap_importance = sorted(zip(feature_names, mean_shap_values), key=lambda x: x[1], reverse=True)

print(f"\nüîç **SHAP Values for All 21 Features:**")
for feature, value in shap_importance:
    print(f"{feature}: {value:.4f}")

# ‚úÖ Extract Top 3 Features (SHAP)
top_3_features = shap_importance[:3]

print(f"\nüî• **Top 3 Contributing Features (SHAP) for Ensemble Prediction:**")
for feature, value in top_3_features:
    print(f"{feature}: {value:.4f}")




import pickle

# Save the trained model to a file
with open('fetal_ensemble_model.pkl', 'wb') as model_file:
    pickle.dump(ensemble_model, model_file)

print("Model saved successfully!")


  with open("fetal_scaler.pkl", "wb") as scaler_file:
    pickle.dump(scaler, scaler_file)

      with open("fetal_shap_explainer.pkl", "wb") as shap_file:
    pickle.dump(explainer, shap_file)
      
